# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment
There are many different variations of ReLU to account for different siutations/modeling

## Challenging, interesting, or exciting aspects of today's assignment
Lots of math with these activation functions

## Additional resources used 
<To be filled>
