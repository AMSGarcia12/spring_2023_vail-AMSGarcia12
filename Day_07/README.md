# Optimization

## Topics covered in today's module
* Optimization
* Gradident Descent
* Optimizers(SGD, ADAM, etc.)

## Main takeaways from doing today's assignment
Many optimizers work with gradients, to find local minimia/maxima for loss functions (minima mostly)

## Challenging, interesting, or exciting aspects of today's assignment
Calc III was actually worth! I am remembering a lot of the concepts from learning about gradients like local mins/maxes and saddle points.

## Additional resources used 
<To be filled>
